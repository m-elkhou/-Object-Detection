{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" border=0 text-align=\"center\">\n",
    "<tr><td >\n",
    "  <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\" text-align=\"center\">\n",
    "    <img src=\"assets/usmba.png\" width=70px />USMBA\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\">\n",
    "    <img src=\"assets/fsdm.png\" width=75px/>FSDM\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://www.univ-paris13.fr/\">\n",
    "    <img src=\"assets/uspn.png\" width=150px/>USPN</a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"http://www.imperium-media.com/\">\n",
    "    <img src=\"assets/imperium_media.png\" width=70px/>IMPERIUM MEDIA</a>\n",
    "    </td></tr>\n",
    "    \n",
    "<tr align=\"center\" text-align=\"center\"><td>\n",
    "  USMBA\n",
    "</td><td>\n",
    "FSDM\n",
    "</td><td>\n",
    "USPN\n",
    "</td><td>\n",
    "IMPERIUM MEDIA\n",
    "    </td></tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Deep Learning: A Review (2001-2020)\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/m-elkhou/Object-Detection/blob/master/object_detection.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/m-elkhou/Object-Detection/blob/master/object_detection.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>\n",
    "<img src=\"assets/1.png\"  />\n",
    "\n",
    "## Abstract\n",
    "Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures.\n",
    "\n",
    "En raison de la relation étroite de la détection d'objets avec l'analyse vidéo et la compréhension d'images, elle a attiré beaucoup d'attention de la recherche ces dernières années.\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "To gain a complete image understanding, we should not only concentrate on classifying different images, but also try to precisely estimate the concepts and locations of objects contained in each image. This task is referred as object detection , which usually consists of different subtasks such as face detection , pedestrian detection and skeleton detection . As one of the fundamental computer vision problems, object detection is able to provide valuable information for semantic understanding of images and videos, and is related to many applications, including image classification, human behavior analysis , face recognition and autonomous driving . \n",
    "Meanwhile, Inheriting from neural networks and related learning systems, the progress in these fields will develop neural network algorithms, and will also have great impacts on object detection techniques which can be considered as learning systems. However, due to large variations in viewpoints, poses, occlusions and lighting conditions, it’s difficult to perfectly accomplish object detection with an additional object localization task. So much attention has been attracted to this field in recent years .\n",
    "The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification).\n",
    "So the pipeline of traditional object detection models can be mainly divided into three stages: informative region selection, feature extraction and classification.\n",
    "\n",
    "Informative region selection. As different objects may appear in any positions of the image and have different aspect ratios or sizes, it is a natural choice to scan the whole image with a multi-scale sliding window. Although this exhaustive strategy can find out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate windows, it is computationally expensive and produces too many redundant windows. However, if only a fixed number of sliding window templates are applied, unsatisfactory regions may be produced.\n",
    "\n",
    "Feature extraction : To recognize different objects, we need to extract visual features which can provide a semantic and robust representation. SIFT , HOG  and Haar-like features are the representative ones. This is due to the fact that these features can produce representations associated with complex cells in human brain . However, due to the diversity of appearances, illumination conditions and backgrounds, it’s difficult to manually design a robust feature descriptor to perfectly describe all kinds of objects.\n",
    "\n",
    "Classification. Besides, a classifier is needed to distinguish a target object from all the other categories and to make the representations more hierarchical, semantic and informative for visual recognition. Usually, the Supported Vector Machine (SVM) , AdaBoost and Deformable Part-based Model (DPM) are good choices. Among these classifiers, the DPM is a flexible model by combining object parts with deformation cost to handle severe deformations. In DPM, with the aid of a graphical model, carefully designed low-level features and kinematically inspired part decompositions are combined. And discriminative learning of graphical models allows for building high -precision part-based models for a variety of object classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first efficient Face Detector (Viola-Jones Algorithm, 2001)\n",
    "\n",
    "- An efficient algorithm for face detection was invented by Paul Viola & Michael Jones \n",
    "- Their demo showed faces being detected in real time on a webcam feed.\n",
    "- Was the most stunning demonstration of computer vision and its potential at the time. \n",
    "- Soon, it was implemented in OpenCV & face detection became synonymous with Viola and Jones algorithm.\n",
    "<!-- https://raw.githubusercontent.com/m-elkhou/Object-Detection/master/assets/1.png -->\n",
    "![alt text](assets/vg1.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](assets/vg2.jpg \"Logo Title Text 1\")\n",
    "\n",
    "### Much more efficient detection technique (Histograms of Oriented Gradients, 2005)\n",
    "\n",
    "- Navneet Dalal and Bill Triggs invented \"HOG\" for pedestrian detection\n",
    "- Their feature descriptor, Histograms of Oriented Gradients (HOG), significantly outperformed existing algorithms in this task\n",
    "- Handcoded features, just like before\n",
    "\n",
    "- For every single pixel, we want to look at the pixels that directly surrounding it:\n",
    "\n",
    "![Alt Text](assets/hog1.gif)\n",
    "\n",
    "- Goal is, how dark is current pixel compared to surrounding pixels?\n",
    "- We will then draw an arrow showing in which direction the image is getting darker:\n",
    "\n",
    "![Alt Text](assets/hog2.gif)\n",
    "\n",
    "- We repeat that process for every single pixel in the image\n",
    "- Every pixel is replaced by an arrow. These arrows are called gradients\n",
    "- Gradients show the flow from light to dark across the entire image:\n",
    "\n",
    "![Alt Text](assets/hog3.gif)\n",
    "\n",
    "- We'll break up the image into small squares of 16x16 pixels each\n",
    "- In each square, we’ll count up how many gradients point in each major direction\n",
    "- Then we’ll replace that square in the image with the arrow directions that were the strongest.\n",
    "- End result? Original image converted into simple representation that captures basic structure of a face in a simple way:\n",
    "- Detecting faces means find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n",
    "\n",
    "![Alt Text](assets/hog4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of image processing\n",
    "\n",
    "This time our focus will be to automatically label all the shapes in an image and find out where each of them are, down to the pixel. This type of task is  called “object segmentation”. During your exploration of computer vision you may have also come across terms like “object recognition”, “class segmentation”, and “object detection”. These all sound similar and can be confusing at first, but seeing what they do helps clear it up. Below are examples of what kind of information we get from each of the four types. Tasks become more difficult as we move from left to right.\n",
    "\n",
    "![](https://patrickwasp.com/wp-content/uploads/2018/04/compter_vision_problems-2-1024x341.png)\n",
    "\n",
    "- **Object recognition** tells us what is in the image, but not where or how much.\n",
    "- **Class segmentation** adds position information to the different types of objects in the image. \n",
    "- **Object detection** separates out each object with a rough bounding box. \n",
    "- **Object segmentation**. It gives every shape a clear boundary, which can also be used to create the results from the previous three.\n",
    "\n",
    "With a simple dataset like the one we’re using here, we could probably use old school computer vision ideas like Hough (pronounced Huff) circle and line detection or template matching to get pretty good results. But by using deep learning we don’t have to change our approach much to get the same type of results on nearly any type of image dataset. And all without having to think about which exact features we’re looking for. It’s almost magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deep Learning Era begins (2012)\n",
    "### Convolutional Neural Networks CNN\n",
    "\n",
    "“NN”s are neural networks. They’re an idea inspired by how we imagined biological neurons worked. A neural network is a collection of connected neurons and each neuron outputs a signal depending on its inputs and internal parameters. When we train a neural network, we adjust neuron internal parameters to create the outputs we expect.\n",
    "\n",
    "![](https://patrickwasp.com/wp-content/uploads/2018/04/neural_net2-768x377.jpeg)\n",
    "\n",
    "The “C” stands for “convolutional”. CNNs were designed specifically for  learning with images, but are otherwise similar to standard neural networks. They learn filters that slide (“convolve”) across and down images in small sections at time, instead of going through the entire image at once. CNNs use less parameters and memory than regular neural networks, which allows them to work on much larger images than a traditional neural network.\n",
    "\n",
    "- Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky's CNN's performance during ImageNet\n",
    "\n",
    "![Alt Text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-65-638.jpg?cb=1455889178)\n",
    "\n",
    "While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*bGTawFxQwzc5yV1_szDrwQ.png)\n",
    "\n",
    "In classification, there’s generally an image with a single object as the focus and the task is to say what that image is\n",
    "\n",
    "![Alt Text](https://www.learnopencv.com/wp-content/uploads/2019/06/image-classification-vs-object-detection.png)\n",
    "\n",
    "But when we look at the world around us, we carry out far more complex task\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*NdwfHMrW3rpj5SW_VQtWVw.png)\n",
    "\n",
    "We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!\n",
    "\n",
    "Can CNNs help us with such complex tasks? Yes.\n",
    "\n",
    "![Alt Text](https://irenelizihui.files.wordpress.com/2016/02/cnn2.png)\n",
    "\n",
    "![Alt Text](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "- We can take a classifier like VGGNet or Inception and turn it into an object detector by sliding a small window across the image\n",
    "- At each step you run the classifier to get a prediction of what sort of object is inside the current window. \n",
    "- Using a sliding window gives several hundred or thousand predictions for that image, but you only keep the ones the classifier is the most certain about.\n",
    "- This approach works but it’s obviously going to be very slow, since you need to run the classifier many times.\n",
    "\n",
    "![](https://patrickwasp.com/wp-content/uploads/2018/04/padding_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Region-based Convolutional Network (R-CNN)\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*ZQ03Ib84bYioFKoho5HnKg.png)\n",
    "\n",
    "- R-CNN creates bounding boxes, or region proposals, using a process called Selective Search \n",
    "- At a high level, Selective Search looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/0*Sdj6sKDRQyZpO6oH.)\n",
    "\n",
    "1. Generate a set of proposals for bounding boxes.\n",
    "2. Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.\n",
    "3. Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.\n",
    "\n",
    "Region-based Convolutional Network (R-CNN)\n",
    "The first models intuitively begin with the region search and then perform the classification. In R-CNN, the selective search method developed by J.R.R. Uijlings and al. (2012) is an alternative to exhaustive search in an image to capture object location. It initializes small regions in an image and merges them with a hierarchical grouping. Thus the final group is a box containing the entire image. The detected regions are merged according to a variety of color spaces and similarity metrics. The output is a few number of region proposals which could contain an object by merging small regions.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1282/1*RUjYe8yqo7nKAG2lNd2mbw.png)\n",
    "Region-based Convolution Network (R-CNN). Each region proposal feeds a CNN to extract a features vector, possible objects are detected using multiple SVM classifiers and a linear regressor modifies the coordinates of the bounding box. Source: [J. Xu’s Blog](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Region-based Convolutional Network (Fast R-CNN)\n",
    "\n",
    "The purpose of the Fast Region-based Convolutional Network (Fast R-CNN) developed by R. Girshick (2015) is to reduce the time consumption related to the high number of models necessary to analyse all region proposals.\n",
    "\n",
    "A main CNN with multiple convolutional layers is taking the entire image as input instead of using a CNN for each region proposals (R-CNN). Region of Interests (RoIs) are detected with the selective search method applied on the produced feature maps. Formally, the feature maps size is reduced using a RoI pooling layer to get valid Region of Interests with fixed heigh and width as hyperparameters. Each RoI layer feeds fully-connected layers¹ creating a features vector. The vector is used to predict the observed object with a softmax classifier and to adapt bounding box localizations with a linear regressor.\n",
    "\n",
    "The best Fast R-CNNs have reached mAp scores of 70.0% for the 2007 PASCAL VOC test dataset, 68.8% for the 2010 PASCAL VOC test dataset and 68.4% for the 2012 PASCAL VOC test dataset.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*U95lm-Jkwkpy3p8X6IOKlQ.png)\n",
    "The entire image feeds a CNN model to detect RoI on the feature maps. Each region is separated using a RoI pooling layer and it feeds fully-connected layers. This vector is used by a softmax classifier to detect the object and by a linear regressor to modify the coordinates of the bounding box. Source: [J. Xu’s Blog](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Region-based Convolutional Network (Faster R-CNN)\n",
    "\n",
    "Region proposals detected with the selective search method were still necessary in the previous model, which is computationally expensive. S. Ren and al. (2016) have introduced Region Proposal Network (RPN) to directly generate region proposals, predict bounding boxes and detect objects. The Faster Region-based Convolutional Network (Faster R-CNN) is a combination between the RPN and the Fast R-CNN model.\n",
    "\n",
    "A CNN model takes as input the entire image and produces feature maps. A window of size 3x3 slides all the feature maps and outputs a features vector linked to two fully-connected layers, one for box-regression and one for box-classification. Multiple region proposals are predicted by the fully-connected layers. A maximum of k regions is fixed thus the output of the box-regression layer has a size of 4k (coordinates of the boxes, their height and width) and the output of the box-classification layer a size of 2k (“objectness” scores to detect an object or not in the box). The k region proposals detected by the sliding window are called anchors.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1210/1*a8c0DGOsEEAtxBafLO1Zbw.png)\n",
    "Detecting the anchor boxes for a single 3x3 window. Source: [S. Ren and al. (2016)](https://arxiv.org/pdf/1506.01497.pdf)\n",
    "\n",
    "\n",
    "When the anchor boxes are detected, they are selected by applying a threshold over the “objectness” score to keep only the relevant boxes. These anchor boxes and the feature maps computed by the initial CNN model feeds a Fast R-CNN model.\n",
    "\n",
    "Faster R-CNN uses RPN to avoid the selective search method, it accelerates the training and testing processes, and improve the performances. The RPN uses a pre-trained model over the ImageNet dataset for classification and it is fine-tuned on the PASCAL VOC dataset. Then the generated region proposals with anchor boxes are used to train the Fast R-CNN. This process is iterative.\n",
    "\n",
    "The best Faster R-CNNs have obtained mAP scores of 78.8% over the 2007 PASCAL VOC test dataset and 75.9% over the 2012 PASCAL VOC test dataset. They have been trained with PASCAL VOC and COCO datasets. One of these models² is 34 times faster than the Fast R-CNN using the selective search method.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1360/0*HlLHTPDyaJZHLnSl.png)\n",
    "The entire image feeds a CNN model to produce anchor boxes as region proposals with a confidence to contain an object. A Fast R-CNN is used taking as inputs the feature maps and the region proposals. For each box, it produces probabilities to detect each object and correction over the location of the box. Source: [J. Xu’s Blog](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region-based Fully Convolutional Network (R-FCN)\n",
    "Fast and Faster R-CNN methodologies consist in detecting region proposals and recognize an object in each region. The Region-based Fully Convolutional Network (R-FCN) released by J. Dai and al. (2016) is a model with only convolutional layers³ allowing complete backpropagation for training and inference. The authors have merged the two basic steps in a single model to take into account simultaneously the object detection (location invariant) and its position (location variant).\n",
    "\n",
    "A ResNet-101 model takes the initial image as input. The last layer outputs feature maps, each one is specialized in the detection of a category at some location. For example, one feature map is specialized in the detection of a cat, another one in a banana and so on. Such feature maps are called position-sensitive score maps because they take into account the spatial localization of a particular object. It consists of k*k*(C+1) score maps where k is the size of the score map, and C the number of classes. All these maps form the score bank. Basically, we create patches that can recognize part of an object. For example, for k=3, we can recognize 3x3 parts of an object.\n",
    "\n",
    "In parallel, we need to run a RPN to generate Region of Interest (RoI). Finally, we cut each RoI in bins and we check them against the score bank. If enough of these parts are activated, then the patch vote ‘yes’, I recognized the object.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*JFtFIzpDhb3KsN1jran6yA.png)\n",
    "![Alt Text](https://miro.medium.com/max/2000/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg)\n",
    "The input image feeds a ResNet model to produce feature maps. A RPN model detects the Region of Interests and a score is computed for each region to determine the most likely object if there is one. \n",
    "Source: [J. Dai and al. (2016)](https://arxiv.org/pdf/1605.06409.pdf).\n",
    "\n",
    "\n",
    "J. Dai and al. (2016) have detailed an example displayed below. The figures show the reaction of a R-FCN model specialized in detecting a person. For a RoI in the center of the image (Figure 3), the subregions in the feature maps are specific to the patterns associated to a person. Thus they vote for ‘yes, there is a person at this location’. In the Figure 4, the RoI is shifted to the right and it is no longer centred on the person. The subregions in the feature maps do not agree on the person detection, thus they vote ‘no, there is no person at this location’.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*492ImpC37vSxbGupMbpYnQ.png)\n",
    "Source: [J. Dai and al. (2016)](https://arxiv.org/pdf/1605.06409.pdf).\n",
    "\n",
    "The best R-FCNs have reached mAP scores of 83.6% for the 2007 PASCAL VOC test dataset and 82.0%, they have been trained with the 2007, 2012 PASCAL VOC datasets and the COCO dataset. Over the test-dev dataset of the 2015 COCO challenge, they have had a score of 53.2% for an IoU = 0.5 and a score of 31.5% for the official mAP metric. The authors noticed that the R-FCN is 2.5–20 times faster than the Faster R-CNN counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/2560/1*0V2fYKOROa4nCuj3Mi3DgQ.jpeg)\n",
    "\n",
    "Central to Computer Vision is the process of segmentation, which divides whole images into pixel groupings which can then be labelled and classified.\n",
    "\n",
    "Particularly, Semantic Segmentation tries to semantically understand the role of each pixel in the image (e.g. is it a car, a motorbike, or some other type of class?). For example, in the picture above, apart from recognizing the person, the road, the cars, the trees, etc., we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.\n",
    "\n",
    "As with other computer vision tasks, CNNs have had enormous success on segmentation problems. One of the popular initial approaches was patch classification through a sliding window, where each pixel was separately classified into classes using a patch of images around it. This, however, is very inefficient computationally because we don’t reuse the shared features between overlapping patches.\n",
    "\n",
    "The solution, instead, is UC Berkeley’s Fully Convolutional Networks (FCN), which popularized end-to-end CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for images of any size and was also much faster compared to the patch classification approach. Almost all subsequent approaches to semantic segmentation adopted this paradigm.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*_k5SCYeFy43b_CFv_zFimQ.jpeg)\n",
    "\n",
    "However, one problem remains: convolutions at original image resolution will be very expensive. To deal with this, FCN uses downsampling and upsampling inside the network. The downsampling layer is known as striped convolution, while the upsampling layer is known as transposed convolution.\n",
    "\n",
    "Despite the upsampling/downsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. SegNet is a more memory efficient architecture than FCN that uses-max pooling and an encoder-decoder framework. In SegNet, shortcut/skip connections are introduced from higher resolution feature maps to improve the coarseness of upsampling/downsampling.\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*1-ulho5NzNJhq6YNR9KREg.jpeg)\n",
    "\n",
    "Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as [Dilated Convolutions](https://arxiv.org/pdf/1511.07122.pdf), [DeepLab](https://arxiv.org/pdf/1412.7062.pdf), and [RefineNet](https://arxiv.org/pdf/1611.06612.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/2096/1*pDJ1P9Rv-jcas51SZsVt4A.jpeg)\n",
    "\n",
    "Beyond Semantic Segmentation, Instance Segmentation segments different instances of classes, such as labelling 5 cars with 5 different colors. In classification, there’s generally an image with a single object as the focus and the task is to say what that image is. But in order to segment instances, we need to carry out far more complex tasks. We see complicated sights with multiple overlapping objects and different backgrounds, and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!\n",
    "\n",
    "\n",
    "So far, we’ve seen how to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes. Can we extend such techniques to locate exact pixels of each object instead of just bounding boxes? This instance segmentation problem is explored at Facebook AI using an architecture known as Mask R-CNN.\n",
    "![Alt Text](https://miro.medium.com/max/1000/1*ClYLqVgNwZP_nUON061x_w.jpeg)\n",
    "\n",
    "Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straightforward Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel-level segmentation?\n",
    "\n",
    "Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch is a Fully Convolutional Network on top of a CNN-based feature map. Given the CNN Feature Map as the input, the network outputs a matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/1384/1*QgOk_xUmBM-_MlWSXBK-Dg.jpeg)\n",
    "\n",
    "Additionally, when run without modifications on the original Faster R-CNN architecture, the regions of the feature map selected by RoIPool (Region of Interests Pool) were slightly misaligned from the regions of the original image. Since image segmentation requires pixel-level specificity, unlike bounding boxes, this naturally led to inaccuracies. Mask R-CNN solves this problem by adjusting RoIPool to be more precisely aligned using a method known as RoIAlign (Region of Interests Align). Essentially, RoIAlign uses bilinear interpolation to avoid error in rounding, which causes inaccuracies in detection and segmentation.\n",
    "\n",
    "Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/2000/1*fbDDJ5z8q5xaZ4BhiQGDIw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade Mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Shot Detector (SSD)\n",
    "Our final model is SSD, which stands for Single-Shot Detector. Like R-FCN, it provides enormous speed gains over Faster R-CNN, but does so in a markedly different manner.\n",
    "\n",
    "Our first two models performed region proposals and region classifications in two separate steps. First, they used a region proposal network to generate regions of interest; next, they used either fully-connected layers or position-sensitive convolutional layers to classify those regions. SSD does the two in a “single shot,” simultaneously predicting the bounding box and the class as it processes the image.\n",
    "\n",
    "Concretely, given an input image and a set of ground truth labels, SSD does the following:\n",
    "\n",
    "- Pass the image through a series of convolutional layers, yielding several sets of feature maps at different scales (e.g. 10x10, then 6x6, then 3x3, etc.)\n",
    "- For each location in each of these feature maps, use a 3x3 convolutional filter to evaluate a small set of default bounding boxes. These default bounding boxes are essentially equivalent to Faster R-CNN’s anchor boxes.\n",
    "- For each box, simultaneously predict a) the bounding box offset and b) the class probabilities\n",
    "- During training, match the ground truth box with these predicted boxes based on IoU. The best predicted box will be labeled a “positive,” along with all other boxes that have an IoU with the truth >0.5.\n",
    "\n",
    "SSD sounds straightforward, but training it has a unique challenge. With the previous two models, the region proposal network ensured that everything we tried to classify had some minimum probability of being an “object.” With SSD, however, we skip that filtering step. We classify and draw bounding boxes from every single position in the image, using multiple different shapes, at several different scales. As a result, we generate a much greater number of bounding boxes than the other models, and nearly all of the them are negative examples.\n",
    "\n",
    "To fix this imbalance, SSD does two things. Firstly, it uses non-maximum suppression to group together highly-overlapping boxes into a single box. In other words, if four boxes of similar shapes, sizes, etc. contain the same dog, NMS would keep the one with the highest confidence and discard the rest. Secondly, the model uses a technique called hard negative mining to balance classes during training. In hard negative mining, only a subset of the negative examples with the highest training loss (i.e. false positives) are used at each iteration of training. SSD keeps a 3:1 ratio of negatives to positives.\n",
    "\n",
    "Its architecture looks like this:\n",
    "![Alt Text](https://miro.medium.com/max/2000/1*p-lSawysBsiBzlcWZ9_UMw.png)\n",
    "\n",
    "As I mentioned above, there are “extra feature layers” at the end that scale down in size. These varying-size feature maps help capture objects of different sizes. For example, here is SSD in action:\n",
    "![Alt Text](https://miro.medium.com/max/1400/1*JuhjYUWXgfxMMoa4SIKLkA.png)\n",
    "\n",
    "In smaller feature maps (e.g. 4x4), each cell covers a larger region of the image, enabling them to detect larger objects. Region proposal and classification are performed simultaneously: given p object classes, each bounding box is associated with a (4+p)-dimensional vector that outputs 4 box offset coordinates and p class probabilities. In the last step, softmax is again used to classify the object.\n",
    "\n",
    "Ultimately, SSD is not so different from the first two models. It simply skips the “region proposal” step, instead considering every single bounding box in every location of the image simultaneously with its classification. Because SSD does everything in one shot, it is the fastest of the three models, and still performs quite comparably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Only Look Once (YOLO)\n",
    "\n",
    "### What is YOLO?\n",
    "\n",
    "- YOLO takes a completely different approach. \n",
    "- It’s not a traditional classifier that is repurposed to be an object detector. \n",
    "- YOLO actually looks at the image just once (hence its name: You Only Look Once) but in a clever way.\n",
    "\n",
    "YOLO divides up the image into a grid of 13 by 13 cells:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Grid@2x.png)\n",
    "\n",
    "- Each of these cells is responsible for predicting 5 bounding boxes. \n",
    "- A bounding box describes the rectangle that encloses an object.\n",
    "- YOLO also outputs a confidence score that tells us how certain it is that the predicted bounding box actually encloses some object.\n",
    "- This score doesn’t say anything about what kind of object is in the box, just if the shape of the box is any good.\n",
    "\n",
    "The predicted bounding boxes may look something like the following (the higher the confidence score, the fatter the box is drawn):\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Boxes@2x.png)\n",
    "\n",
    "- For each bounding box, the cell also predicts a class. \n",
    "- This works just like a classifier: it gives a probability distribution over all the possible classes. \n",
    "- YOLO was trained on the PASCAL VOC dataset, which can detect 20 different classes such as:\n",
    "\n",
    "\n",
    "- bicycle\n",
    "- boat\n",
    "- car\n",
    "- cat\n",
    "- dog\n",
    "- person\n",
    "\n",
    "- The confidence score for the bounding box and the class prediction are combined into one final score that tells us the probability that this bounding box contains a specific type of object. \n",
    "- For example, the big fat yellow box on the left is 85% sure it contains the object “dog”:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Scores@2x.png)\n",
    "\n",
    "- Since there are 13×13 = 169 grid cells and each cell predicts 5 bounding boxes, we end up with 845 bounding boxes in total. \n",
    "- It turns out that most of these boxes will have very low confidence scores, so we only keep the boxes whose final score is 30% or more (you can change this threshold depending on how accurate you want the detector to be).\n",
    "\n",
    "The final prediction is then:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Prediction@2x.png)\n",
    "\n",
    "- From the 845 total bounding boxes we only kept these three because they gave the best results. \n",
    "- But note that even though there were 845 separate predictions, they were all made at the same time — the neural network just ran once. And that’s why YOLO is so powerful and fast.\n",
    "\n",
    "The architecture of YOLO is simple, it’s just a convolutional neural network:\n",
    "\n",
    "![Alt Text](https://i.imgur.com/QH0CvRN.png)\n",
    "\n",
    "This neural network only uses standard layer types: convolution with a 3×3 kernel and max-pooling with a 2×2 kernel. No fancy stuff. There is no fully-connected layer in YOLOv2.\n",
    "\n",
    "The very last convolutional layer has a 1×1 kernel and exists to reduce the data to the shape 13×13×125. This 13×13 should look familiar: that is the size of the grid that the image gets divided into.\n",
    "\n",
    "So we end up with 125 channels for every grid cell. These 125 numbers contain the data for the bounding boxes and the class predictions. Why 125? Well, each grid cell predicts 5 bounding boxes and a bounding box is described by 25 data elements:\n",
    "\n",
    "- x, y, width, height for the bounding box’s rectangle\n",
    "- the confidence score\n",
    "- the probability distribution over the classes\n",
    "\n",
    "Using YOLO is simple: you give it an input image (resized to 416×416 pixels), it goes through the convolutional network in a single pass, and comes out the other end as a 13×13×125 tensor describing the bounding boxes for the grid cells. All you need to do then is compute the final scores for the bounding boxes and throw away the ones scoring lower than 30%.\n",
    "\n",
    "### Improvements to YOLO v1\n",
    "\n",
    "YoLO v2 vs YoLO v1\n",
    "\n",
    "- Speed (45 frames per second — better than realtime)\n",
    "- Network understands generalized object representation (This allowed them to train the network on real world images and predictions on artwork was still fairly accurate).\n",
    "- faster version (with smaller architecture) — 155 frames per sec but is less accurate.\n",
    "\n",
    "Paper here\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf\n",
    "\n",
    "### Code Walkthrough & demo\n",
    "\n",
    "1. Using pretrained network\n",
    "2. Training on your own dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectron2\n",
    "\n",
    "<img src=\"\n",
    "data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgZGF0YS1uYW1lPSJMYXllciAxIiB4bWxucz0iaHR0cDovL3d3dy53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\" width=\"300\"/>\n",
    "\n",
    "### Object Detection with PyTorch\n",
    "Detectron2 is Facebook AI Research's next generation software system\n",
    "that implements state-of-the-art object detection algorithms.\n",
    "It is a ground-up rewrite of the previous version,\n",
    "[Detectron](https://github.com/facebookresearch/Detectron/),\n",
    "and it originates from [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png\"/>\n",
    "</div>\n",
    "\n",
    "Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.\n",
    "\n",
    "At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440), [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434), and [Group Normalization](https://arxiv.org/abs/1803.08494).\n",
    "\n",
    "The goal of Detectron is to provide a high-quality, high-performance\n",
    "codebase for object detection *research*. It is designed to be flexible in order\n",
    "to support rapid implementation and evaluation of novel research. Detectron\n",
    "includes implementations of the following object detection algorithms:\n",
    "\n",
    "- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*\n",
    "- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*\n",
    "- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "- [RPN](https://arxiv.org/abs/1506.01497)\n",
    "- [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "- [R-FCN](https://arxiv.org/abs/1605.06409)\n",
    "\n",
    "using the following backbone network architectures:\n",
    "\n",
    "- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)\n",
    "- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)\n",
    "- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)\n",
    "- [VGG16](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Additional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- R-CNN: https://arxiv.org/abs/1311.2524\n",
    "- Fast R-CNN: https://arxiv.org/abs/1504.08083\n",
    "- Faster R-CNN: https://arxiv.org/abs/1506.01497\n",
    "- Mask R-CNN: https://arxiv.org/abs/1703.06870\n",
    "\n",
    "\n",
    "- [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440).\n",
    "  Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He.\n",
    "  Tech report, arXiv, Dec. 2017.\n",
    "- [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370).\n",
    "  Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick.\n",
    "  Tech report, arXiv, Nov. 2017.\n",
    "- [Non-Local Neural Networks](https://arxiv.org/abs/1711.07971).\n",
    "  Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\n",
    "  Tech report, arXiv, Nov. 2017.\n",
    "- [Mask R-CNN](https://arxiv.org/abs/1703.06870).\n",
    "  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\n",
    "  IEEE International Conference on Computer Vision (ICCV), 2017.\n",
    "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).\n",
    "  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.\n",
    "  IEEE International Conference on Computer Vision (ICCV), 2017.\n",
    "- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).\n",
    "  Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.\n",
    "  Tech report, arXiv, June 2017.\n",
    "- [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333).\n",
    "  Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.\n",
    "  Tech report, arXiv, Apr. 2017.\n",
    "- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).\n",
    "  Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431).\n",
    "  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "- [R-FCN: Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409).\n",
    "  Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.\n",
    "  Conference on Neural Information Processing Systems (NIPS), 2016.\n",
    "- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).\n",
    "  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n",
    "- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)\n",
    "  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
    "  Conference on Neural Information Processing Systems (NIPS), 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
