{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" border=0 ><tr><th >\n",
    "  <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\"><img src=\"assets/usmba.png\" width=70px /></a>\n",
    "</th><th>\n",
    "  <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\"><img src=\"assets/fsdm.png\" width=75px/></a>\n",
    "</th><th>\n",
    "  <a target=\"_blank\"  href=\"https://www.univ-paris13.fr/\"><img src=\"assets/uspn.png\" width=150px/></a>\n",
    "</th><th>\n",
    "  <a target=\"_blank\"  href=\"http://www.imperium-media.com/\"><img src=\"assets/imperium_media.png\" width=70px/></a>\n",
    "</th></tr><tr><td style=\"text-align:center;\">\n",
    "    <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\">USMBA</a>\n",
    "</td><td style=\"text-align:center;\">\n",
    "    <a target=\"_blank\"  href=\"http://www.fsdmfes.ac.ma/\">FSDM</a>\n",
    "</td><td style=\"text-align:center;\">\n",
    "    <a target=\"_blank\"  href=\"https://www.univ-paris13.fr/\">USPN</a>\n",
    "</td><td style=\"text-align:center;\">\n",
    "    <a target=\"_blank\"  href=\"http://www.imperium-media.com/\">IMPERIUM MEDIA</a>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Deep Learning: A Review (2001-2020)\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/m-elkhou/Object-Detection/blob/master/object_detection.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/m-elkhou/Object-Detection/blob/master/object_detection.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>\n",
    "<div align=\"center\"><img src=\"assets/1.png\" /></div>\n",
    "\n",
    "## Abstract\n",
    "Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures.\n",
    "\n",
    "En raison de la relation étroite de la détection d'objets avec l'analyse vidéo et la compréhension d'images, elle a attiré beaucoup d'attention de la recherche ces dernières années.\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "To gain a complete image understanding, we should not only concentrate on classifying different images, but also try to precisely estimate the concepts and locations of objects contained in each image. This task is referred as object detection , which usually consists of different subtasks such as face detection , pedestrian detection and skeleton detection . As one of the fundamental computer vision problems, object detection is able to provide valuable information for semantic understanding of images and videos, and is related to many applications, including image classification, human behavior analysis , face recognition and autonomous driving . \n",
    "Meanwhile, Inheriting from neural networks and related learning systems, the progress in these fields will develop neural network algorithms, and will also have great impacts on object detection techniques which can be considered as learning systems. However, due to large variations in viewpoints, poses, occlusions and lighting conditions, it’s difficult to perfectly accomplish object detection with an additional object localization task. So much attention has been attracted to this field in recent years .\n",
    "The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification).\n",
    "So the pipeline of traditional object detection models can be mainly divided into three stages: informative region selection, feature extraction and classification.\n",
    "\n",
    "**Informative region selection**. As different objects may appear in any positions of the image and have different aspect ratios or sizes, it is a natural choice to scan the whole image with a multi-scale sliding window. Although this exhaustive strategy can find out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate windows, it is computationally expensive and produces too many redundant windows. However, if only a fixed number of sliding window templates are applied, unsatisfactory regions may be produced.\n",
    "\n",
    "**Feature extraction** : To recognize different objects, we need to extract visual features which can provide a semantic and robust representation. SIFT , HOG  and Haar-like features are the representative ones. This is due to the fact that these features can produce representations associated with complex cells in human brain . However, due to the diversity of appearances, illumination conditions and backgrounds, it’s difficult to manually design a robust feature descriptor to perfectly describe all kinds of objects.\n",
    "\n",
    "**Classification**. Besides, a classifier is needed to distinguish a target object from all the other categories and to make the representations more hierarchical, semantic and informative for visual recognition. Usually, the Supported Vector Machine (SVM) , AdaBoost and Deformable Part-based Model (DPM) are good choices. Among these classifiers, the DPM is a flexible model by combining object parts with deformation cost to handle severe deformations. In DPM, with the aid of a graphical model, carefully designed low-level features and kinematically inspired part decompositions are combined. And discriminative learning of graphical models allows for building high -precision part-based models for a variety of object classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first efficient Face Detector (Viola-Jones Algorithm, 2001)\n",
    "\n",
    "- An efficient algorithm for face detection was invented by Paul Viola & Michael Jones \n",
    "- Their demo showed faces being detected in real time on a webcam feed.\n",
    "- Was the most stunning demonstration of computer vision and its potential at the time. \n",
    "- Soon, it was implemented in OpenCV & face detection became synonymous with Viola and Jones algorithm.\n",
    "<!-- https://raw.githubusercontent.com/m-elkhou/Object-Detection/master/assets/1.png -->\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/vg1.png\"/>\n",
    "    <img src=\"assets/vg2.jpg\"/>\n",
    "</div>\n",
    "\n",
    "### Much more efficient detection technique (Histograms of Oriented Gradients, 2005)\n",
    "\n",
    "- Navneet Dalal and Bill Triggs invented \"HOG\" for pedestrian detection\n",
    "- Their feature descriptor, Histograms of Oriented Gradients (HOG), significantly outperformed existing algorithms in this task\n",
    "- Handcoded features, just like before\n",
    "\n",
    "- For every single pixel, we want to look at the pixels that directly surrounding it:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/hog1.gif\"/>\n",
    "</div>\n",
    "\n",
    "- Goal is, how dark is current pixel compared to surrounding pixels?\n",
    "- We will then draw an arrow showing in which direction the image is getting darker:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/hog2.gif\"/>\n",
    "</div>\n",
    "\n",
    "- We repeat that process for every single pixel in the image\n",
    "- Every pixel is replaced by an arrow. These arrows are called gradients\n",
    "- Gradients show the flow from light to dark across the entire image:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/hog3.gif\" width=600/>\n",
    "</div>\n",
    "\n",
    "- We'll break up the image into small squares of 16x16 pixels each\n",
    "- In each square, we’ll count up how many gradients point in each major direction\n",
    "- Then we’ll replace that square in the image with the arrow directions that were the strongest.\n",
    "- End result? Original image converted into simple representation that captures basic structure of a face in a simple way:\n",
    "- Detecting faces means find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/hog4.png\" width=700/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of image processing\n",
    "\n",
    "This time our focus will be to automatically label all the shapes in an image and find out where each of them are, down to the pixel. This type of task is  called “object segmentation”. During your exploration of computer vision you may have also come across terms like “object recognition”, “class segmentation”, and “object detection”. These all sound similar and can be confusing at first, but seeing what they do helps clear it up. Below are examples of what kind of information we get from each of the four types. Tasks become more difficult as we move from left to right.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/compter_vision_problems.png\"/>\n",
    "</div>\n",
    "\n",
    "- **Object recognition** tells us what is in the image, but not where or how much.\n",
    "- **Class segmentation** adds position information to the different types of objects in the image. \n",
    "- **Object detection** separates out each object with a rough bounding box. \n",
    "- **Object segmentation**. It gives every shape a clear boundary, which can also be used to create the results from the previous three.\n",
    "\n",
    "With a simple dataset like the one we’re using here, we could probably use old school computer vision ideas like Hough (pronounced Huff) circle and line detection or template matching to get pretty good results. But by using deep learning we don’t have to change our approach much to get the same type of results on nearly any type of image dataset. And all without having to think about which exact features we’re looking for. It’s almost magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks CNN\n",
    "> The Deep Learning Era begins (2012)\n",
    "\n",
    "“NN”s are neural networks. They’re an idea inspired by how we imagined biological neurons worked. A neural network is a collection of connected neurons and each neuron outputs a signal depending on its inputs and internal parameters. When we train a neural network, we adjust neuron internal parameters to create the outputs we expect.\n",
    "\n",
    "![](assets/nn.jpeg)\n",
    "\n",
    "The “C” stands for “convolutional”. CNNs were designed specifically for  learning with images, but are otherwise similar to standard neural networks. They learn filters that slide (“convolve”) across and down images in small sections at time, instead of going through the entire image at once. CNNs use less parameters and memory than regular neural networks, which allows them to work on much larger images than a traditional neural network.\n",
    "\n",
    "- Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky's CNN's performance during ImageNet\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/cnn.jpg\"/>\n",
    "</div>\n",
    "\n",
    "While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/cnn1.png\"/>\n",
    "</div>\n",
    "\n",
    "In classification, there’s generally an image with a single object as the focus and the task is to say what that image is\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/image-classification-vs-object-detection.png\"  width=720/>\n",
    "</div>\n",
    "\n",
    "But when we look at the world around us, we carry out far more complex task\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/cnn3.png\"/>\n",
    "</div>\n",
    "\n",
    "We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!\n",
    "\n",
    "Can CNNs help us with such complex tasks? Yes.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/cnn2.png\" width=620/>\n",
    "    <img src=\"assets/cnn4.png\" width=620/>\n",
    "</div>\n",
    "\n",
    "- We can take a classifier like VGGNet or Inception and turn it into an object detector by sliding a small window across the image\n",
    "- At each step you run the classifier to get a prediction of what sort of object is inside the current window. \n",
    "- Using a sliding window gives several hundred or thousand predictions for that image, but you only keep the ones the classifier is the most certain about.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/padding_strides.gif\"/>\n",
    "</div>\n",
    "\n",
    "- This approach works but it’s obviously going to be very slow, since you need to run the classifier many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Region-based Convolutional Network (R-CNN)\n",
    "\n",
    "The first models intuitively begin with the region search and then perform the classification. In R-CNN, the **Selective Search** method developed by [J.R.R. Uijlings and al. (2012)](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) is an alternative to exhaustive search in an image to capture object location. It initializes small regions in an image and merges them with a hierarchical grouping. Thus the final group is a box containing the entire image. The detected regions are merged according to a variety of color spaces and similarity metrics. The output is a few number of region proposals which could contain an object by merging small regions.\n",
    "\n",
    "![Alt Text](assets/rcnn.png)\n",
    "\n",
    "The R-CNN model ([Ross Girshick and al. (2013-2014)](https://arxiv.org/abs/1311.2524)) combines the selective search method to detect region proposals and deep learning to find out the object in these regions. \n",
    "\n",
    "To bypass the problem of selecting a huge number of regions, Ross Girshick et al. proposed a method where we use selective search to extract just 2000 regions from the image and he called them **region proposals**. Therefore, now, instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm.\n",
    "\n",
    "Let’s first understand what selective search is and how it identifies the different regions. There are basically four regions that form an object: varying scales, colors, textures, and enclosure. Selective search identifies these patterns in the image and based on that, proposes various regions. Here is a brief overview of how selective search works:\n",
    "\n",
    "1. It first takes an image as input:\n",
    "![](assets/rcn1.png)\n",
    "2. Then, it generates initial sub-segmentations so that we have many candidate regions from this image:\n",
    "![](assets/rcnn2.png)\n",
    "3. The technique then combines the similar regions to form a larger region (based on color similarity, texture similarity, size similarity, and shape compatibility):\n",
    "![](assets/rcnn3.png)\n",
    "4. Finally, these regions then produce the final object locations (Region of Interest).\n",
    "\n",
    "Below is a succint summary of the steps followed in RCNN to detect objects:\n",
    "\n",
    "![Alt Text](assets/rcnn4.png)\n",
    "The architecture of R-CNN. (Image source: [R. Girshick and al. (2014)](https://arxiv.org/abs/1311.2524))\n",
    "\n",
    "1. Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 region proposals\n",
    "2. Run a convolutional neural net (CNN) on top of each of these region proposals\n",
    "3. Take the output of each CNN and feed it into a) an SVM to classify the region and b) a linear regressor to tighten the bounding box of the object, if such an object exists.\n",
    "\n",
    "These 3 steps are illustrated in the image below:\n",
    "![Alt Text](assets/rcnn5.png)\n",
    "R-CNN. Each region proposal feeds a CNN to extract a features vector, possible objects are detected using multiple SVM classifiers and a linear regressor modifies the coordinates of the bounding box. Source: [J. Xu’s Blog](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)\n",
    "\n",
    "These 2000 candidate region proposals are warped into a square and fed into a convolutional neural network that produces a 4096-dimensional feature vector as output. The CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image and the extracted features are fed into an SVM to classify the presence of the object within that candidate region proposal. In addition to predicting the presence of an object within the region proposals, the algorithm also predicts four values which are offset values to increase the precision of the bounding box. For example, given a region proposal, the algorithm would have predicted the presence of a person but the face of that person within that region proposal could’ve been cut in half. Therefore, the offset values help in adjusting the bounding box of the region proposal.\n",
    "\n",
    "In other words, we first propose regions, then extract features, and then classify those regions based on their features. In essence, we have turned object detection into an image classification problem. R-CNN was very intuitive, but very slow.\n",
    "\n",
    "#### Problems with R-CNN\n",
    "- It still takes a huge amount of time to train the network as you would have to classify 2000 region proposals per image.\n",
    "- It cannot be implemented real time as it takes around 47 seconds for each test image.\n",
    "- The selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Region-based Convolutional Network (Fast R-CNN)\n",
    "\n",
    "The purpose of the Fast Region-based Convolutional Network (Fast R-CNN) developed by [R. Girshick, (2015)](https://arxiv.org/pdf/1504.08083.pdf) is to reduce the time consumption related to the high number of models necessary to analyse all region proposals.\n",
    "Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object).\n",
    "\n",
    "![The architecture of Fast R-CNN](assets/fast_r-cnn.png)\n",
    "The architecture of Fast R-CNN. (Image source: [Girshick, 2015](https://arxiv.org/pdf/1504.08083.pdf))\n",
    "\n",
    "Ross Girshick, the author of RCNN, came up with this idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use a **RoI pooling** layer to reshape all the proposed regions into a fixed size, so that it can be fed into a fully connected network.\n",
    "\n",
    "#### RoI pooling :\n",
    "It is a type of max pooling to convert features in the projected region of the image of any size, h x w, into a small fixed window, H x W. The input region is divided into H x W grids, approximately every subwindow of size h/H x w/W. Then apply max-pooling in each grid.\n",
    "<img src=\"assets/roi-pooling.png\" width=620/>\n",
    "\n",
    "Fig. RoI pooling (Image source: [Stanford CS231n slides.](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf))\n",
    "\n",
    "Let’s break this down into steps to simplify the concept:\n",
    "\n",
    "1. We follow the now well-known step of taking an image as input:\n",
    "![](assets/fast_r-cnn1.png)\n",
    "2. This image is passed to a ConvNet which returns the Regions of Interest accordingly:\n",
    "![](assets/fast_r-cnn2.png)\n",
    "3. Then we apply the RoI pooling layer on the extracted regions of interest to make sure all the regions are of the same size:\n",
    "![](assets/fast_r-cnn3.png)\n",
    "4. Finally, these regions are passed on to a fully connected network which classifies them, as well as returns the bounding boxes using softmax and linear regression layers simultaneously:\n",
    "![](assets/fast_r-cnn4.png)\n",
    "\n",
    "This is how Fast RCNN resolves two major issues of RCNN, i.e., passing one instead of 2,000 regions per image to the ConvNet, and using one instead of three different models for extracting features, classification and generating bounding boxes.\n",
    "\n",
    "The best Fast R-CNNs have reached mAp scores of 70.0% for the 2007 PASCAL VOC test dataset, 68.8% for the 2010 PASCAL VOC test dataset and 68.4% for the 2012 PASCAL VOC test dataset.\n",
    "![](assets/fast_r-cnn5.png)\n",
    "\n",
    "### Problems with Fast RCNN\n",
    "But even Fast RCNN has certain problem areas. It also uses selective search as a proposal method to find the Regions of Interest, which is a slow and time consuming process. It takes around 2 seconds per image to detect objects, which is much better compared to RCNN. But when we consider large real-life datasets, then even a Fast RCNN doesn’t look so fast anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Region-based Convolutional Network (Faster R-CNN)\n",
    "\n",
    "Region proposals detected with the selective search method were still necessary in the previous model, which is computationally expensive. S. Ren and al. (2016) have introduced Region Proposal Network (RPN) to directly generate region proposals, predict bounding boxes and detect objects. The Faster Region-based Convolutional Network (Faster R-CNN) is a combination between the RPN and the Fast R-CNN model.\n",
    "\n",
    "A CNN model takes as input the entire image and produces feature maps. A window of size 3x3 slides all the feature maps and outputs a features vector linked to two fully-connected layers, one for box-regression and one for box-classification. Multiple region proposals are predicted by the fully-connected layers. A maximum of k regions is fixed thus the output of the box-regression layer has a size of 4k (coordinates of the boxes, their height and width) and the output of the box-classification layer a size of 2k (“objectness” scores to detect an object or not in the box). The k region proposals detected by the sliding window are called anchors.\n",
    "\n",
    "![Alt Text](assets/faster_rcnn.png)\n",
    "Detecting the anchor boxes for a single 3x3 window. Source: [S. Ren and al. (2016)](https://arxiv.org/pdf/1506.01497.pdf)\n",
    "\n",
    "\n",
    "When the anchor boxes are detected, they are selected by applying a threshold over the “objectness” score to keep only the relevant boxes. These anchor boxes and the feature maps computed by the initial CNN model feeds a Fast R-CNN model.\n",
    "\n",
    "Faster R-CNN uses RPN to avoid the selective search method, it accelerates the training and testing processes, and improve the performances. The RPN uses a pre-trained model over the ImageNet dataset for classification and it is fine-tuned on the PASCAL VOC dataset. Then the generated region proposals with anchor boxes are used to train the Fast R-CNN. This process is iterative.\n",
    "\n",
    "The best Faster R-CNNs have obtained mAP scores of 78.8% over the 2007 PASCAL VOC test dataset and 75.9% over the 2012 PASCAL VOC test dataset. They have been trained with PASCAL VOC and COCO datasets. One of these models² is 34 times faster than the Fast R-CNN using the selective search method.\n",
    "\n",
    "![Alt Text](assets/faster_rcnn1.png)\n",
    "The entire image feeds a CNN model to produce anchor boxes as region proposals with a confidence to contain an object. A Fast R-CNN is used taking as inputs the feature maps and the region proposals. For each box, it produces probabilities to detect each object and correction over the location of the box. Source: [J. Xu’s Blog](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region-based Fully Convolutional Network (R-FCN)\n",
    "Fast and Faster R-CNN methodologies consist in detecting region proposals and recognize an object in each region. The Region-based Fully Convolutional Network (R-FCN) released by J. Dai and al. (2016) is a model with only convolutional layers³ allowing complete backpropagation for training and inference. The authors have merged the two basic steps in a single model to take into account simultaneously the object detection (location invariant) and its position (location variant).\n",
    "\n",
    "A ResNet-101 model takes the initial image as input. The last layer outputs feature maps, each one is specialized in the detection of a category at some location. For example, one feature map is specialized in the detection of a cat, another one in a banana and so on. Such feature maps are called position-sensitive score maps because they take into account the spatial localization of a particular object. It consists of k*k*(C+1) score maps where k is the size of the score map, and C the number of classes. All these maps form the score bank. Basically, we create patches that can recognize part of an object. For example, for k=3, we can recognize 3x3 parts of an object.\n",
    "\n",
    "In parallel, we need to run a RPN to generate Region of Interest (RoI). Finally, we cut each RoI in bins and we check them against the score bank. If enough of these parts are activated, then the patch vote ‘yes’, I recognized the object.\n",
    "\n",
    "<img width=620 src=\"assets/r-fcn.png\"/>\n",
    "<img width=620 src=\"assets/r-fcn1.png\"/>\n",
    "\n",
    "The input image feeds a ResNet model to produce feature maps. A RPN model detects the Region of Interests and a score is computed for each region to determine the most likely object if there is one. \n",
    "Source: [J. Dai and al. (2016)](https://arxiv.org/pdf/1605.06409.pdf).\n",
    "\n",
    "\n",
    "J. Dai and al. (2016) have detailed an example displayed below. The figures show the reaction of a R-FCN model specialized in detecting a person. For a RoI in the center of the image (Figure 3), the subregions in the feature maps are specific to the patterns associated to a person. Thus they vote for ‘yes, there is a person at this location’. In the Figure 4, the RoI is shifted to the right and it is no longer centred on the person. The subregions in the feature maps do not agree on the person detection, thus they vote ‘no, there is no person at this location’.\n",
    "\n",
    "<img width=620 src=\"assets/r-fcn3.png\"/>\n",
    "\n",
    "Source: [J. Dai and al. (2016)](https://arxiv.org/pdf/1605.06409.pdf).\n",
    "\n",
    "The best R-FCNs have reached mAP scores of 83.6% for the 2007 PASCAL VOC test dataset and 82.0%, they have been trained with the 2007, 2012 PASCAL VOC datasets and the COCO dataset. Over the test-dev dataset of the 2015 COCO challenge, they have had a score of 53.2% for an IoU = 0.5 and a score of 31.5% for the official mAP metric. The authors noticed that the R-FCN is 2.5–20 times faster than the Faster R-CNN counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "![Alt Text](assets/semantic_segmentation.jpeg)\n",
    "\n",
    "Central to Computer Vision is the process of segmentation, which divides whole images into pixel groupings which can then be labelled and classified.\n",
    "\n",
    "Particularly, Semantic Segmentation tries to semantically understand the role of each pixel in the image (e.g. is it a car, a motorbike, or some other type of class?). For example, in the picture above, apart from recognizing the person, the road, the cars, the trees, etc., we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.\n",
    "\n",
    "As with other computer vision tasks, CNNs have had enormous success on segmentation problems. One of the popular initial approaches was patch classification through a sliding window, where each pixel was separately classified into classes using a patch of images around it. This, however, is very inefficient computationally because we don’t reuse the shared features between overlapping patches.\n",
    "\n",
    "The solution, instead, is UC Berkeley’s Fully Convolutional Networks (FCN), which popularized end-to-end CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for images of any size and was also much faster compared to the patch classification approach. Almost all subsequent approaches to semantic segmentation adopted this paradigm.\n",
    "\n",
    "![Alt Text](assets/semantic_segmentation1.jpeg)\n",
    "\n",
    "However, one problem remains: convolutions at original image resolution will be very expensive. To deal with this, FCN uses downsampling and upsampling inside the network. The downsampling layer is known as striped convolution, while the upsampling layer is known as transposed convolution.\n",
    "\n",
    "Despite the upsampling/downsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. SegNet is a more memory efficient architecture than FCN that uses-max pooling and an encoder-decoder framework. In SegNet, shortcut/skip connections are introduced from higher resolution feature maps to improve the coarseness of upsampling/downsampling.\n",
    "\n",
    "![Alt Text](assets/semantic_segmentation2.jpeg)\n",
    "\n",
    "Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as [Dilated Convolutions](https://arxiv.org/pdf/1511.07122.pdf), [DeepLab](https://arxiv.org/pdf/1412.7062.pdf), and [RefineNet](https://arxiv.org/pdf/1611.06612.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation\n",
    "\n",
    "![Alt Text](assets/instance_segmentation.jpeg)\n",
    "\n",
    "Beyond Semantic Segmentation, Instance Segmentation segments different instances of classes, such as labelling 5 cars with 5 different colors. In classification, there’s generally an image with a single object as the focus and the task is to say what that image is. But in order to segment instances, we need to carry out far more complex tasks. We see complicated sights with multiple overlapping objects and different backgrounds, and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!\n",
    "\n",
    "\n",
    "So far, we’ve seen how to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes. Can we extend such techniques to locate exact pixels of each object instead of just bounding boxes? This instance segmentation problem is explored at Facebook AI using an architecture known as Mask R-CNN.\n",
    "![Alt Text](assets/instance_segmentation1.jpeg)\n",
    "\n",
    "Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straightforward Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel-level segmentation?\n",
    "\n",
    "Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch is a Fully Convolutional Network on top of a CNN-based feature map. Given the CNN Feature Map as the input, the network outputs a matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).\n",
    "\n",
    "<img src=\"assets/mask_r-cnn.png\" width=620 />\n",
    "<img src=\"assets/mask_r-cnn2.png\" width=620 />\n",
    "\n",
    "The MaskR-CNN framework for instance segmentation. (source: [He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf))\n",
    "\n",
    "Additionally, when run without modifications on the original Faster R-CNN architecture, the regions of the feature map selected by RoIPool (Region of Interests Pool) were slightly misaligned from the regions of the original image. Since image segmentation requires pixel-level specificity, unlike bounding boxes, this naturally led to inaccuracies. Mask R-CNN solves this problem by adjusting RoIPool to be more precisely aligned using a method known as RoIAlign (Region of Interests Align). Essentially, RoIAlign uses bilinear interpolation to avoid error in rounding, which causes inaccuracies in detection and segmentation.\n",
    "\n",
    "Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:\n",
    "\n",
    "![Alt Text](assets/mask_r-cnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://missinglink.ai/guides/tensorflow/building-faster-r-cnn-on-tensorflow-introduction-and-examples/\n",
    "https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade Mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Shot Detector (SSD)\n",
    "Our final model is SSD, which stands for Single-Shot Detector. Like R-FCN, it provides enormous speed gains over Faster R-CNN, but does so in a markedly different manner.\n",
    "\n",
    "Our first two models performed region proposals and region classifications in two separate steps. First, they used a region proposal network to generate regions of interest; next, they used either fully-connected layers or position-sensitive convolutional layers to classify those regions. SSD does the two in a “single shot,” simultaneously predicting the bounding box and the class as it processes the image.\n",
    "\n",
    "Concretely, given an input image and a set of ground truth labels, SSD does the following:\n",
    "\n",
    "- Pass the image through a series of convolutional layers, yielding several sets of feature maps at different scales (e.g. 10x10, then 6x6, then 3x3, etc.)\n",
    "- For each location in each of these feature maps, use a 3x3 convolutional filter to evaluate a small set of default bounding boxes. These default bounding boxes are essentially equivalent to Faster R-CNN’s anchor boxes.\n",
    "- For each box, simultaneously predict a) the bounding box offset and b) the class probabilities\n",
    "- During training, match the ground truth box with these predicted boxes based on IoU. The best predicted box will be labeled a “positive,” along with all other boxes that have an IoU with the truth >0.5.\n",
    "\n",
    "SSD sounds straightforward, but training it has a unique challenge. With the previous two models, the region proposal network ensured that everything we tried to classify had some minimum probability of being an “object.” With SSD, however, we skip that filtering step. We classify and draw bounding boxes from every single position in the image, using multiple different shapes, at several different scales. As a result, we generate a much greater number of bounding boxes than the other models, and nearly all of the them are negative examples.\n",
    "\n",
    "To fix this imbalance, SSD does two things. Firstly, it uses non-maximum suppression to group together highly-overlapping boxes into a single box. In other words, if four boxes of similar shapes, sizes, etc. contain the same dog, NMS would keep the one with the highest confidence and discard the rest. Secondly, the model uses a technique called hard negative mining to balance classes during training. In hard negative mining, only a subset of the negative examples with the highest training loss (i.e. false positives) are used at each iteration of training. SSD keeps a 3:1 ratio of negatives to positives.\n",
    "\n",
    "Its architecture looks like this:\n",
    "![Alt Text](assets/ssd.png)\n",
    "\n",
    "As I mentioned above, there are “extra feature layers” at the end that scale down in size. These varying-size feature maps help capture objects of different sizes. For example, here is SSD in action:\n",
    "![Alt Text](assets/ssd1.png)\n",
    "\n",
    "In smaller feature maps (e.g. 4x4), each cell covers a larger region of the image, enabling them to detect larger objects. Region proposal and classification are performed simultaneously: given p object classes, each bounding box is associated with a (4+p)-dimensional vector that outputs 4 box offset coordinates and p class probabilities. In the last step, softmax is again used to classify the object.\n",
    "\n",
    "Ultimately, SSD is not so different from the first two models. It simply skips the “region proposal” step, instead considering every single bounding box in every location of the image simultaneously with its classification. Because SSD does everything in one shot, it is the fastest of the three models, and still performs quite comparably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Only Look Once (YOLO)\n",
    "\n",
    "### What is YOLO?\n",
    "\n",
    "- YOLO takes a completely different approach. \n",
    "- It’s not a traditional classifier that is repurposed to be an object detector. \n",
    "- YOLO actually looks at the image just once (hence its name: You Only Look Once) but in a clever way.\n",
    "\n",
    "YOLO divides up the image into a grid of 13 by 13 cells:\n",
    "\n",
    "![Alt Text](assets/yolo1.png)\n",
    "\n",
    "- Each of these cells is responsible for predicting 5 bounding boxes. \n",
    "- A bounding box describes the rectangle that encloses an object.\n",
    "- YOLO also outputs a confidence score that tells us how certain it is that the predicted bounding box actually encloses some object.\n",
    "- This score doesn’t say anything about what kind of object is in the box, just if the shape of the box is any good.\n",
    "\n",
    "The predicted bounding boxes may look something like the following (the higher the confidence score, the fatter the box is drawn):\n",
    "\n",
    "![Alt Text](assets/yolo2.png)\n",
    "\n",
    "- For each bounding box, the cell also predicts a class. \n",
    "- This works just like a classifier: it gives a probability distribution over all the possible classes. \n",
    "- YOLO was trained on the PASCAL VOC dataset, which can detect 20 different classes such as:\n",
    "\n",
    "\n",
    "- bicycle\n",
    "- boat\n",
    "- car\n",
    "- cat\n",
    "- dog\n",
    "- person\n",
    "\n",
    "- The confidence score for the bounding box and the class prediction are combined into one final score that tells us the probability that this bounding box contains a specific type of object. \n",
    "- For example, the big fat yellow box on the left is 85% sure it contains the object “dog”:\n",
    "\n",
    "![Alt Text](assets/yolo3.png)\n",
    "\n",
    "- Since there are 13×13 = 169 grid cells and each cell predicts 5 bounding boxes, we end up with 845 bounding boxes in total. \n",
    "- It turns out that most of these boxes will have very low confidence scores, so we only keep the boxes whose final score is 30% or more (you can change this threshold depending on how accurate you want the detector to be).\n",
    "\n",
    "The final prediction is then:\n",
    "\n",
    "![Alt Text](assets/yolo4.png)\n",
    "\n",
    "- From the 845 total bounding boxes we only kept these three because they gave the best results. \n",
    "- But note that even though there were 845 separate predictions, they were all made at the same time — the neural network just ran once. And that’s why YOLO is so powerful and fast.\n",
    "\n",
    "The architecture of YOLO is simple, it’s just a convolutional neural network:\n",
    "\n",
    "<img src=\"assets/yolo5.png\" width=500/>\n",
    "\n",
    "This neural network only uses standard layer types: convolution with a 3×3 kernel and max-pooling with a 2×2 kernel. No fancy stuff. There is no fully-connected layer in YOLOv2.\n",
    "\n",
    "The very last convolutional layer has a 1×1 kernel and exists to reduce the data to the shape 13×13×125. This 13×13 should look familiar: that is the size of the grid that the image gets divided into.\n",
    "\n",
    "So we end up with 125 channels for every grid cell. These 125 numbers contain the data for the bounding boxes and the class predictions. Why 125? Well, each grid cell predicts 5 bounding boxes and a bounding box is described by 25 data elements:\n",
    "\n",
    "- x, y, width, height for the bounding box’s rectangle\n",
    "- the confidence score\n",
    "- the probability distribution over the classes\n",
    "\n",
    "Using YOLO is simple: you give it an input image (resized to 416×416 pixels), it goes through the convolutional network in a single pass, and comes out the other end as a 13×13×125 tensor describing the bounding boxes for the grid cells. All you need to do then is compute the final scores for the bounding boxes and throw away the ones scoring lower than 30%.\n",
    "\n",
    "### Improvements to YOLO v1\n",
    "\n",
    "YoLO v2 vs YoLO v1\n",
    "\n",
    "- Speed (45 frames per second — better than realtime)\n",
    "- Network understands generalized object representation (This allowed them to train the network on real world images and predictions on artwork was still fairly accurate).\n",
    "- faster version (with smaller architecture) — 155 frames per sec but is less accurate.\n",
    "\n",
    "Paper here\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf\n",
    "\n",
    "### Code Walkthrough & demo\n",
    "\n",
    "1. Using pretrained network\n",
    "2. Training on your own dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=w0tDDFip7KM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectron2\n",
    "\n",
    "<img src=\"assets/detectron2.png\" width=\"300\"/>\n",
    "\n",
    "### Object Detection with PyTorch\n",
    "Detectron2 is Facebook AI Research's next generation software system\n",
    "that implements state-of-the-art object detection algorithms.\n",
    "It is a ground-up rewrite of the previous version,\n",
    "[Detectron](https://github.com/facebookresearch/Detectron/),\n",
    "and it originates from [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/).\n",
    "\n",
    "<img src=\"assets/detectron.png\"/>\n",
    "\n",
    "Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.\n",
    "\n",
    "At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440), [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434), and [Group Normalization](https://arxiv.org/abs/1803.08494).\n",
    "\n",
    "The goal of Detectron is to provide a high-quality, high-performance\n",
    "codebase for object detection *research*. It is designed to be flexible in order\n",
    "to support rapid implementation and evaluation of novel research. Detectron\n",
    "includes implementations of the following object detection algorithms:\n",
    "\n",
    "- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*\n",
    "- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*\n",
    "- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "- [RPN](https://arxiv.org/abs/1506.01497)\n",
    "- [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "- [R-FCN](https://arxiv.org/abs/1605.06409)\n",
    "\n",
    "using the following backbone network architectures:\n",
    "\n",
    "- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)\n",
    "- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)\n",
    "- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)\n",
    "- [VGG16](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Additional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- R-CNN: https://arxiv.org/abs/1311.2524\n",
    "- Fast R-CNN: https://arxiv.org/abs/1504.08083\n",
    "- Faster R-CNN: https://arxiv.org/abs/1506.01497\n",
    "- Mask R-CNN: https://arxiv.org/abs/1703.06870\n",
    "\n",
    "\n",
    "- [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440).\n",
    "  Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He.\n",
    "  Tech report, arXiv, Dec. 2017.\n",
    "- [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370).\n",
    "  Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick.\n",
    "  Tech report, arXiv, Nov. 2017.\n",
    "- [Non-Local Neural Networks](https://arxiv.org/abs/1711.07971).\n",
    "  Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\n",
    "  Tech report, arXiv, Nov. 2017.\n",
    "- [Mask R-CNN](https://arxiv.org/abs/1703.06870).\n",
    "  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\n",
    "  IEEE International Conference on Computer Vision (ICCV), 2017.\n",
    "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).\n",
    "  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.\n",
    "  IEEE International Conference on Computer Vision (ICCV), 2017.\n",
    "- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).\n",
    "  Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.\n",
    "  Tech report, arXiv, June 2017.\n",
    "- [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333).\n",
    "  Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.\n",
    "  Tech report, arXiv, Apr. 2017.\n",
    "- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).\n",
    "  Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431).\n",
    "  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "- [R-FCN: Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409).\n",
    "  Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.\n",
    "  Conference on Neural Information Processing Systems (NIPS), 2016.\n",
    "- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).\n",
    "  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
    "  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n",
    "- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)\n",
    "  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
    "  Conference on Neural Information Processing Systems (NIPS), 2015.\n",
    "  \n",
    "  \n",
    "- Blog of James Le, 2018 :\n",
    "https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b\n",
    "- Blog of Arthur Ouaknine, 2018 :\n",
    "https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
