{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Deep Learning: A Review (2001-2019)\n",
    "\n",
    "## Abstract\n",
    "Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures.\n",
    "\n",
    "En raison de la relation étroite de la détection d'objets avec l'analyse vidéo et la compréhension d'images, elle a attiré beaucoup d'attention de la recherche ces dernières années.\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "To gain a complete image understanding, we should not only concentrate on classifying different images, but also try to precisely estimate the concepts and locations of objects contained in each image. This task is referred as object detection , which usually consists of different subtasks such as face detection , pedestrian detection and skeleton detection . As one of the fundamental computer vision problems, object detection is able to provide valuable information for semantic understanding of images and videos, and is related to many applications, including image classification, human behavior analysis , face recognition and autonomous driving . \n",
    "Meanwhile, Inheriting from neural networks and related learning systems, the progress in these fields will develop neural network algorithms, and will also have great impacts on object detection techniques which can be considered as learning systems. However, due to large variations in viewpoints, poses, occlusions and lighting conditions, it’s difficult to perfectly accomplish object detection with an additional object localization task. So much attention has been attracted to this field in recent years .\n",
    "The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification).\n",
    "So the pipeline of traditional object detection models can be mainly divided into three stages: informative region selection, feature extraction and classification.\n",
    "\n",
    "Informative region selection. As different objects may appear in any positions of the image and have different aspect ratios or sizes, it is a natural choice to scan the whole image with a multi-scale sliding window. Although this exhaustive strategy can find out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate windows, it is computationally expensive and produces too many redundant windows. However, if only a fixed number of sliding window templates are applied, unsatisfactory regions may be produced.\n",
    "\n",
    "Feature extraction : To recognize different objects, we need to extract visual features which can provide a semantic and robust representation. SIFT , HOG  and Haar-like features are the representative ones. This is due to the fact that these features can produce representations associated with complex cells in human brain . However, due to the diversity of appearances, illumination conditions and backgrounds, it’s difficult to manually design a robust feature descriptor to perfectly describe all kinds of objects.\n",
    "\n",
    "Classification. Besides, a classifier is needed to distinguish a target object from all the other categories and to make the representations more hierarchical, semantic and informative for visual recognition. Usually, the Supported Vector Machine (SVM) , AdaBoost and Deformable Part-based Model (DPM) are good choices. Among these classifiers, the DPM is a flexible model by combining object parts with deformation cost to handle severe deformations. In DPM, with the aid of a graphical model, carefully designed low-level features and kinematically inspired part decompositions are combined. And discriminative learning of graphical models allows for building high -precision part-based models for a variety of object classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first efficient Face Detector (Viola-Jones Algorithm, 2001)\n",
    "\n",
    "- An efficient algorithm for face detection was invented by Paul Viola & Michael Jones \n",
    "- Their demo showed faces being detected in real time on a webcam feed.\n",
    "- Was the most stunning demonstration of computer vision and its potential at the time. \n",
    "- Soon, it was implemented in OpenCV & face detection became synonymous with Viola and Jones algorithm.\n",
    "\n",
    "![alt text](assets/Fig-2-The-proposed-approach-a-Viola-Jones-algorithm-b-Active-Shape-Models-with.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://ars.els-cdn.com/content/image/1-s2.0-S2468067216300116-gr1.jpg \"Logo Title Text 1\")\n",
    "\n",
    "### Much more efficient detection technique (Histograms of Oriented Gradients, 2005)\n",
    "\n",
    "- Navneet Dalal and Bill Triggs invented \"HOG\" for pedestrian detection\n",
    "- Their feature descriptor, Histograms of Oriented Gradients (HOG), significantly outperformed existing algorithms in this task\n",
    "- Handcoded features, just like before\n",
    "\n",
    "- For every single pixel, we want to look at the pixels that directly surrounding it:\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1440/1*RZS05e_5XXQdofdRx1GvPA.gif)\n",
    "\n",
    "- Goal is, how dark is current pixel compared to surrounding pixels?\n",
    "- We will then draw an arrow showing in which direction the image is getting darker:\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1440/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif)\n",
    "\n",
    "- We repeat that process for every single pixel in the image\n",
    "- Every pixel is replaced by an arrow. These arrows are called gradients\n",
    "- Gradients show the flow from light to dark across the entire image:\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1440/1*oTdaElx_M-_z9c_iAwwqcw.gif)\n",
    "\n",
    "- We'll break up the image into small squares of 16x16 pixels each\n",
    "- In each square, we’ll count up how many gradients point in each major direction\n",
    "- Then we’ll replace that square in the image with the arrow directions that were the strongest.\n",
    "- End result? Original image converted into simple representation that captures basic structure of a face in a simple way:\n",
    "- Detecting faces means find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1440/1*6xgev0r-qn4oR88FrW6fiA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Deep Learning Era begins (2012)\n",
    "### Convolutional Neural Networks CNN\n",
    "- Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky's CNN's performance during ImageNet\n",
    "\n",
    "![Alt Text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-65-638.jpg?cb=1455889178)\n",
    "\n",
    "While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*bGTawFxQwzc5yV1_szDrwQ.png)\n",
    "\n",
    "In classification, there’s generally an image with a single object as the focus and the task is to say what that image is\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*8GVucX9yhnL21KCtcyFDRQ.png)\n",
    "\n",
    "But when we look at the world around us, we carry out far more complex task\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*NdwfHMrW3rpj5SW_VQtWVw.png)\n",
    "\n",
    "We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!\n",
    "\n",
    "Can CNNs help us with such complex tasks? Yes.\n",
    "\n",
    "![Alt Text](https://irenelizihui.files.wordpress.com/2016/02/cnn2.png)\n",
    "\n",
    "![Alt Text](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "- We can take a classifier like VGGNet or Inception and turn it into an object detector by sliding a small window across the image\n",
    "- At each step you run the classifier to get a prediction of what sort of object is inside the current window. \n",
    "- Using a sliding window gives several hundred or thousand predictions for that image, but you only keep the ones the classifier is the most certain about.\n",
    "- This approach works but it’s obviously going to be very slow, since you need to run the classifier many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### A better approach, R-CNN\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*ZQ03Ib84bYioFKoho5HnKg.png)\n",
    "\n",
    "- R-CNN creates bounding boxes, or region proposals, using a process called Selective Search \n",
    "- At a high level, Selective Search looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.\n",
    "\n",
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/0*Sdj6sKDRQyZpO6oH.)\n",
    "\n",
    "1. Generate a set of proposals for bounding boxes.\n",
    "2. Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.\n",
    "3. Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.\n",
    "\n",
    "Region-based Convolutional Network (R-CNN)\n",
    "The first models intuitively begin with the region search and then perform the classification. In R-CNN, the selective search method developed by J.R.R. Uijlings and al. (2012) is an alternative to exhaustive search in an image to capture object location. It initializes small regions in an image and merges them with a hierarchical grouping. Thus the final group is a box containing the entire image. The detected regions are merged according to a variety of color spaces and similarity metrics. The output is a few number of region proposals which could contain an object by merging small regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some improvements to R-CNN\n",
    "\n",
    "- R-CNN: https://arxiv.org/abs/1311.2524\n",
    "- Fast R-CNN: https://arxiv.org/abs/1504.08083\n",
    "- Faster R-CNN: https://arxiv.org/abs/1506.01497\n",
    "- Mask R-CNN: https://arxiv.org/abs/1703.06870\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But YOLO takes a different approach\n",
    "\n",
    "### What is YOLO?\n",
    "\n",
    "- YOLO takes a completely different approach. \n",
    "- It’s not a traditional classifier that is repurposed to be an object detector. \n",
    "- YOLO actually looks at the image just once (hence its name: You Only Look Once) but in a clever way.\n",
    "\n",
    "YOLO divides up the image into a grid of 13 by 13 cells:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Grid@2x.png)\n",
    "\n",
    "- Each of these cells is responsible for predicting 5 bounding boxes. \n",
    "- A bounding box describes the rectangle that encloses an object.\n",
    "- YOLO also outputs a confidence score that tells us how certain it is that the predicted bounding box actually encloses some object.\n",
    "- This score doesn’t say anything about what kind of object is in the box, just if the shape of the box is any good.\n",
    "\n",
    "The predicted bounding boxes may look something like the following (the higher the confidence score, the fatter the box is drawn):\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Boxes@2x.png)\n",
    "\n",
    "- For each bounding box, the cell also predicts a class. \n",
    "- This works just like a classifier: it gives a probability distribution over all the possible classes. \n",
    "- YOLO was trained on the PASCAL VOC dataset, which can detect 20 different classes such as:\n",
    "\n",
    "- bicycle\n",
    "- boat\n",
    "- car\n",
    "- cat\n",
    "- dog\n",
    "- person\n",
    "\n",
    "- The confidence score for the bounding box and the class prediction are combined into one final score that tells us the probability that this bounding box contains a specific type of object. \n",
    "- For example, the big fat yellow box on the left is 85% sure it contains the object “dog”:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Scores@2x.png)\n",
    "\n",
    "- Since there are 13×13 = 169 grid cells and each cell predicts 5 bounding boxes, we end up with 845 bounding boxes in total. \n",
    "- It turns out that most of these boxes will have very low confidence scores, so we only keep the boxes whose final score is 30% or more (you can change this threshold depending on how accurate you want the detector to be).\n",
    "\n",
    "The final prediction is then:\n",
    "\n",
    "![Alt Text](http://machinethink.net/images/yolo/Prediction@2x.png)\n",
    "\n",
    "- From the 845 total bounding boxes we only kept these three because they gave the best results. \n",
    "- But note that even though there were 845 separate predictions, they were all made at the same time — the neural network just ran once. And that’s why YOLO is so powerful and fast.\n",
    "\n",
    "The architecture of YOLO is simple, it’s just a convolutional neural network:\n",
    "\n",
    "![Alt Text](https://i.imgur.com/QH0CvRN.png)\n",
    "\n",
    "This neural network only uses standard layer types: convolution with a 3×3 kernel and max-pooling with a 2×2 kernel. No fancy stuff. There is no fully-connected layer in YOLOv2.\n",
    "\n",
    "The very last convolutional layer has a 1×1 kernel and exists to reduce the data to the shape 13×13×125. This 13×13 should look familiar: that is the size of the grid that the image gets divided into.\n",
    "\n",
    "So we end up with 125 channels for every grid cell. These 125 numbers contain the data for the bounding boxes and the class predictions. Why 125? Well, each grid cell predicts 5 bounding boxes and a bounding box is described by 25 data elements:\n",
    "\n",
    "- x, y, width, height for the bounding box’s rectangle\n",
    "- the confidence score\n",
    "- the probability distribution over the classes\n",
    "\n",
    "Using YOLO is simple: you give it an input image (resized to 416×416 pixels), it goes through the convolutional network in a single pass, and comes out the other end as a 13×13×125 tensor describing the bounding boxes for the grid cells. All you need to do then is compute the final scores for the bounding boxes and throw away the ones scoring lower than 30%.\n",
    "\n",
    "### Improvements to YOLO v1\n",
    "\n",
    "YoLO v2 vs YoLO v1\n",
    "\n",
    "- Speed (45 frames per second — better than realtime)\n",
    "- Network understands generalized object representation (This allowed them to train the network on real world images and predictions on artwork was still fairly accurate).\n",
    "- faster version (with smaller architecture) — 155 frames per sec but is less accurate.\n",
    "\n",
    "Paper here\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf\n",
    "\n",
    "### Code Walkthrough & demo\n",
    "\n",
    "1. Using pretrained network\n",
    "2. Training on your own dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
