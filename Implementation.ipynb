{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table align=\"center\" >\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                <a  href=\"http://www.fsdmfes.ac.ma/\">\n",
    "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/usmba.png?raw=1\" width=70px />\n",
    "                </a>\n",
    "            </th>\n",
    "            <th>\n",
    "                <a  href=\"http://www.fsdmfes.ac.ma/\">\n",
    "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/fsdm.png?raw=1\" width=75px />\n",
    "                </a>\n",
    "            </th>\n",
    "            <th>\n",
    "                <a href=\"https://www.univ-paris13.fr/\">\n",
    "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/uspn.png?raw=1\" width=150px/>\n",
    "                </a>\n",
    "            </th>\n",
    "            <th>\n",
    "                <a href=\"http://www.imperium-media.com/\">\n",
    "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/imperium_media.png?raw=1\" width=70px/>\n",
    "                </a>\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">\n",
    "                <a   href=\"http://www.fsdmfes.ac.ma/\">USMBA</a>\n",
    "            </td>\n",
    "            <td style=\"text-align:center;\">\n",
    "              <a   href=\"http://www.fsdmfes.ac.ma/\">FSDM</a>\n",
    "            </td>\n",
    "            <td style=\"text-align:center;\">\n",
    "                <a  href=\"https://www.univ-paris13.fr/\">USPN</a>\n",
    "            </td>\n",
    "            <td style=\"text-align:center;\">\n",
    "                <a  href=\"http://www.imperium-media.com/\">IMPERIUM MEDIA</a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#  **Object Detection with Deep Learning** #\n",
    "\n",
    "***\n",
    "\n",
    "<div align=\"left\" >\n",
    "    <a href=\"https://colab.research.google.com/github/m-elkhou/Object-Detection/blob/master/Implementation.ipynb\">\n",
    "        <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\" />\n",
    "    </a>\n",
    "    <a href=\"https://github.com/m-elkhou/Object-Detection/blob/master/Implementation.ipynb\">\n",
    "        <img align=\"left\" src=\"https://badgen.net/badge/icon/Open%20in%20GitHub?icon=github&label\" alt=\"Open in GitHub\" title=\"View source on GitHub\" />\n",
    "    </a>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details:\n",
    "Here, we are going to explain the implementation steps for  different object detection models. To train the model on the custom dataset, standard implementation steps would be as below:\n",
    "1. Data Acquisition\n",
    "2. Data Labelling\n",
    "3. Data Preparation\n",
    "4. Model Training\n",
    "5. Evaluation / Results analysis\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "Data acquisition is a required step when you want to train the model on the custom dataset. It will require more than 100 images with the object for which you want to train the model. Each image should have good quality, contains object/objects at least once (to be detected). No image should be repetitive within the dataset, the more the variety of images, the more extensive the training becomes.\n",
    "\n",
    "Single Shot Detectron (SSD) and Faster R-CNN require images with objects only, but in the case of YOLO, negative samples are also being used for the training and validation.\n",
    "\n",
    "Here, we are going to showcase the demo for the model training, and prediction of the custom object which is the logo of the brand VIVO. We have collected ~200 images for the training.\n",
    "\n",
    "### Data Labelling\n",
    "\n",
    "For the custom dataset, we need to label objects from each of the images. For object detection, the model requires certain details of the objects that are to be detected from the image and those are the X-axis, y-axis, height, and width of the object within the image.\n",
    "\n",
    "Run below command to install labelImg\n",
    "\n",
    "```python\n",
    "pip install labelImg\n",
    "```\n",
    "You can also refer to https://github.com/tzutalin/labelImg for the reference. The labelImg tool provides a user interface that allows a user to draw bounding boxes and captures the x-axis and y-axis along with the height and width of the bounding box. This also provides a feature to label multiple objects within a single image. labelImg stores axis values in a separate XML file at the same location where the image is present.\n",
    "\n",
    "**Data Preparation & Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN:\n",
    "#### Data Preparation:\n",
    "1. Create a text file from the xml file which is generated from the labelImg where txt file where each image path and it’s relative bounding boxes co-ordinate in new line which is in the following format and move it to the cloned repository :\n",
    "```\n",
    "filepath,xmin,ymin,xmax,ymax,class_name\n",
    "```  \n",
    "\n",
    "#### Training\n",
    "\n",
    "1. In order to implement the object detection using Faster R-CNN first we need to clone this GitHub repository:\n",
    "```\n",
    "git clone https://github.com/kbardool/keras-frcnn\n",
    "```\n",
    "2. Move the train_images and test_images folder to the cloned repository\n",
    "3. Now open the terminal in the repository folder and install requirements.txt file.\n",
    "4. We can train the model using Faster R-CNN through executing this line in the terminal:\n",
    "```\n",
    "python train_frcnn.py -o simple -p (path to txt file)\n",
    "```\n",
    "\n",
    "#### Inference:\n",
    "1. Now put the testing images in the folder named test_images and in the same folder of Faster R-CNN and execute the following line in the terminal:\n",
    "python test_frcnn.py -p test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD:\n",
    "\n",
    "We have referred the following link for the implementation of object detection using SSD:\n",
    "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\n",
    "    \n",
    "#### Data Preparation:\n",
    "\n",
    "1. Created .pbtxt file containing label names and it’s id.\n",
    "2. Then we converted all labelImg xml file to csv file using xml_to_csv.py.\n",
    "3. We then converted the CSV file to .record file format using generate_tfrecord.py.\n",
    "\n",
    "#### Training:\n",
    "\n",
    "1. Then we had downloaded the pretrained model of ssd_inception and also created a config file according to tutorial with the following changes in the config file:\n",
    "```config\n",
    "num_classes: 1\n",
    "type: 'ssd_inception_v2' # Set to the name of your chosen pre- \\ trained\n",
    "model\n",
    "fine_tune_checkpoint: \"pre-trained-model/model.ckpt\" # Path to \\ extracted\n",
    "files of pre-trained model\n",
    "train_input_reader: { \\\n",
    "tf_record_input_reader { \\\n",
    "input_path: \"annotations/train.record\" # Path to training \\\n",
    "TFRecord file\n",
    "}\n",
    "label_map_path: \"annotations/label_map.pbtxt\" # Path to label \\ map\n",
    "file\n",
    "}\n",
    "```\n",
    "\n",
    "2. We can train the model through executing this line in the terminal:\n",
    "```\n",
    "python train.py --logtostderr --train_dir=training -- \\ pipeline_config_path=training/ssd_inception_v2_coco.config\n",
    "```\n",
    "\n",
    "#### Inference:\n",
    "1. Copy the TensorFlow/models/research/object_detection/export_inference_graph.py script and paste it straight into your training_demo folder.\n",
    "2. Then using the highest checkpoint file of the model and generate .pb file by executing this line in the terminal:\n",
    "```\n",
    "python export_inference_graph.py --input_type image_tensor -- \\ pipeline_config_path training/ssd_inception_v2_coco.confi -- \\ trained_checkpoint_prefix training/model.ckpt-13302 - \\ output_directory trained-inference- \\ graphs/output_inference_graph_v1.pb\n",
    "```\n",
    "3. Using this object_detection code we can predict the test images for SSD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "# Path to frozen detection graph .pb file\n",
    "PATH_TO_CKPT = '.../frozen_inference_graph.pb'\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = '.../label_map.pbtxt' \n",
    "\n",
    "# Path to image\n",
    "PATH_TO_IMAGE = '.../test_images/394.jpg'\n",
    "\n",
    "# Number of classes the object detector can identify\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# Load the label map.\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = \\ label_map_util.convert_label_map_to_categories(label_map, \\ max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "# Load the Tensorflow model into memory.\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "    sess = tf.Session(graph=detection_graph)\n",
    "    \n",
    "# Input tensor is the image\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular \\ object was detected\n",
    "detection_boxes = \\ detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represents level of confidence for each of the objects.\n",
    "detection_scores = \\ detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "detection_classes = \\ detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "num_detections = \\ detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "image = cv2.imread(PATH_TO_IMAGE)\n",
    "image_expanded = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Perform the actual detection by running the model with the image \\ as input\n",
    "(boxes, scores, classes, num) = sess.run(\n",
    "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "    feed_dict={image_tensor: image_expanded})\n",
    "    \n",
    "vis_util.visualize_boxes_and_labels_on_image_array( \\\n",
    "    image, \\\n",
    "    np.squeeze(boxes), \\\n",
    "    np.squeeze(classes).astype(np.int32), \\\n",
    "    np.squeeze(scores), \\\n",
    "    category_index, \\\n",
    "    use_normalized_coordinates=True, \\\n",
    "    line_thickness=8, \\\n",
    "    min_score_thresh=0.60)\n",
    "\n",
    "# All the results have been drawn on image. Now display the image.\n",
    "cv2.imshow('Object detector', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "In our use case, we have focused on the detection of the logo of the brand VIVO. We have annotated 150 images for the training. Each image has an average of 3 objects. We have captured training images from the different sports events in order to keep the variety of background within the train images. Also, we have captured 50 of the images, with quite a similar background. However, Faster R-CNN and SSD do not consider negative samples (samples without objects) for the training.\n",
    "\n",
    "For the comparative analysis as a first observation, we have trained the object detection model for 2K steps/ iteration on 150 images.\n",
    "\n",
    "Sample training images are as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'samples.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2fb36291d54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"samples.tar.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# tf.extractall()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1571\u001b[0m                     \u001b[0msaved_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1573\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1574\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mReadError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1639\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python37\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'samples.tar.gz'"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "tf = tarfile.open(\"samples.tar.gz\")\n",
    "# tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
